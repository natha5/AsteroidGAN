{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "HSWuwdFI0uqr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CmY_lJmazXLD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rand\n",
        "\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Activation, Conv1D, Conv2D, Dropout\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data pre-processing\n",
        "\n",
        "Due to the large size of dataset, many columns that are not needed and some rows containing null values have already been dropped"
      ],
      "metadata": {
        "id": "31YHj_tf0rBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "data = data.drop(['class', 'epoch_cal'], axis=1)\n",
        "data = data.dropna()\n",
        "data['neo'] = data['neo'].replace(('Y','N'), (1,0))\n",
        "data['pha'] = data['pha'].replace(('Y','N'), (1,0))\n",
        "\n",
        "#num_classes = data['class'].nunique()\n",
        "#classes = to_categorical(data['class'], num_classes)\n",
        "\n",
        "data = data.astype('float32')\n",
        "data /= 255\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "print(data.info)"
      ],
      "metadata": {
        "id": "NaRAER8R0w2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89cd4d4f-5dae-4c66-d800-577b8a2f8612"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(131124, 16)\n",
            "<bound method DataFrame.info of         neo  pha         H  diameter    albedo         e         a         q  \\\n",
            "0       0.0  0.0  0.013333  3.683922  0.000353  0.000298  0.010859  0.010034   \n",
            "1       0.0  0.0  0.016471  2.137255  0.000396  0.000902  0.010878  0.008376   \n",
            "2       0.0  0.0  0.020902  0.967043  0.000839  0.001008  0.010464  0.007775   \n",
            "3       0.0  0.0  0.011765  2.060392  0.001658  0.000348  0.009260  0.008439   \n",
            "4       0.0  0.0  0.027059  0.418427  0.001075  0.000749  0.010094  0.008167   \n",
            "...     ...  ...       ...       ...       ...       ...       ...       ...   \n",
            "573686  0.0  0.0  0.061176  0.017612  0.000094  0.000717  0.011518  0.009411   \n",
            "573687  0.0  0.0  0.060000  0.018020  0.000361  0.000289  0.012456  0.011538   \n",
            "573688  0.0  0.0  0.060392  0.016024  0.000455  0.000326  0.012663  0.011610   \n",
            "573689  0.0  0.0  0.064314  0.012820  0.000408  0.000827  0.012432  0.009811   \n",
            "573690  0.0  0.0  0.065882  0.008361  0.000243  0.000899  0.011454  0.008827   \n",
            "\n",
            "               i        om         w        ad         n        tp_cal  \\\n",
            "0       0.041545  0.314924  0.288618  0.011685  0.000839  79138.937500   \n",
            "1       0.136600  0.678528  1.216480  0.013379  0.000837  79140.085938   \n",
            "2       0.050945  0.666084  0.972809  0.013152  0.000887  79141.664062   \n",
            "3       0.028007  0.407101  0.591092  0.010082  0.001065  79139.257812   \n",
            "4       0.021049  0.555180  1.406464  0.012021  0.000936  79216.929688   \n",
            "...          ...       ...       ...       ...       ...           ...   \n",
            "573686  0.109669  0.674466  0.631966  0.013625  0.000768  79137.734375   \n",
            "573687  0.090955  0.168830  0.812197  0.013374  0.000683  79297.695312   \n",
            "573688  0.064425  0.247204  0.285630  0.013716  0.000666  79256.156250   \n",
            "573689  0.052861  0.314412  0.295110  0.015053  0.000685  79256.945312   \n",
            "573690  0.063265  0.714666  1.132945  0.014080  0.000774  79217.289062   \n",
            "\n",
            "             per      moid  \n",
            "0       6.600572  0.006254  \n",
            "1       6.617298  0.004840  \n",
            "2       6.243191  0.004056  \n",
            "3       5.197775  0.004469  \n",
            "4       5.915351  0.004297  \n",
            "...          ...       ...  \n",
            "573686  7.209965  0.005500  \n",
            "573687  8.108079  0.007620  \n",
            "573688  8.311297  0.007947  \n",
            "573689  8.085117  0.006092  \n",
            "573690  7.149632  0.005193  \n",
            "\n",
            "[131124 rows x 16 columns]>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-ec6d9db84190>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['neo'] = data['neo'].replace(('Y','N'), (1,0))\n",
            "<ipython-input-2-ec6d9db84190>:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['pha'] = data['pha'].replace(('Y','N'), (1,0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training parameters\n"
      ],
      "metadata": {
        "id": "Im19QWilH2OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (16,1)\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS = 'binary_crossentropy'\n",
        "METRICS = 'accuracy'\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 20\n",
        "VERBOSE = 1\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "metadata": {
        "id": "nfLDJUYCNVvI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator model\n",
        "18 outputs as there are 18 columns in the dataset"
      ],
      "metadata": {
        "id": "4yFogldYp9gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator(latent_dim):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(40, activation='relu', input_dim=latent_dim))\n",
        "  model.add(Dense(18, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "594ZrKNrNOVt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator model\n",
        "This is a binary CNN classifier, to determine whether the input is 'real' or not"
      ],
      "metadata": {
        "id": "nfK5IwLMrRPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv1D(64, kernel_size=5, padding='same', activation='relu', input_shape=INPUT_SHAPE))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "yHa3MhT8rQvA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_discriminator()\n",
        "\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slp2HkUaOfmN",
        "outputId": "fbc44da2-ec5f-404f-ace9-a6280b9d898c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 16, 64)            384       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16, 1)             65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 449\n",
            "Trainable params: 449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_real = data\n",
        "y_real = np.ones((131124,1))"
      ],
      "metadata": {
        "id": "15lI4UpzrFUc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate random, fake samples for training discriminator"
      ],
      "metadata": {
        "id": "OZbVrKZ7e_fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fakes():\n",
        "  neo = rand.choice([1,0])\n",
        "  pha = rand.choice([1,0])\n",
        "  diameter = rand.uniform(0,100000)\n",
        "  albedo = rand.uniform(0,1)\n",
        "  e = rand.uniform(0, 0.002)\n",
        "  a = rand.uniform(0,1)\n",
        "  q = rand.uniform(0,1)\n",
        "  i = rand.uniform(0,1)\n",
        "  om = rand.uniform(0,1)\n",
        "  w = rand.uniform(0,2)\n",
        "  ad = rand.uniform(0,0.05)\n",
        "  n = rand.uniform(0,0.01)\n",
        "  tp_cal = rand.uniform(0,100000)\n",
        "  per = rand.uniform(0,10)\n",
        "  moid = rand.uniform(0,0.1)\n",
        "\n",
        "  X = pd.DataFrame([neo, pha, diameter, albedo, e, a, q, i, om, w, ad, n, tp_cal, per, moid])\n",
        "  return X"
      ],
      "metadata": {
        "id": "ZFO_oZ2de-yW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_fake = pd.DataFrame(columns = x_real.columns)\n",
        "for i in range(131124):\n",
        "  # fix below line - needs to add as a new row, not each value from the function as its own separate row\n",
        "  current_fake = generate_fakes()\n",
        "  x_fake = pd.concat([x_fake, pd.Series([current_fake])], ignore_index=True)\n",
        "y_fake = np.zeros(131124)\n",
        "\n",
        "x = pd.concat([x_real, x_fake])\n",
        "y = pd.DataFrame(pd.Series(y_real))\n",
        "y = pd.concat([y, pd.Series(y_fake)])\n"
      ],
      "metadata": {
        "id": "AkmvH66zSjMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_real.shape, y_real.shape)\n",
        "print(y_real)\n",
        "history = model.fit(x_real, y_real, epochs=N_EPOCHS, batch_size=BATCH_SIZE, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
      ],
      "metadata": {
        "id": "9DwYeN-wCuzq",
        "outputId": "31469d5e-f2f5-4d6a-e0a7-5f1fb6a08ac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(131124, 16) (131124, 1)\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " ...\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "Epoch 1/20\n",
            "820/820 [==============================] - 6s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 2/20\n",
            "820/820 [==============================] - 4s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 3/20\n",
            "820/820 [==============================] - 5s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "820/820 [==============================] - 4s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "820/820 [==============================] - 5s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 6/20\n",
            "820/820 [==============================] - 6s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 7/20\n",
            "820/820 [==============================] - 5s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 8/20\n",
            "820/820 [==============================] - 6s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 9/20\n",
            "820/820 [==============================] - 5s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 10/20\n",
            "820/820 [==============================] - 6s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 11/20\n",
            "820/820 [==============================] - 5s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 12/20\n",
            "820/820 [==============================] - 6s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 13/20\n",
            "820/820 [==============================] - 4s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "820/820 [==============================] - 5s 6ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "820/820 [==============================] - 6s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 16/20\n",
            "820/820 [==============================] - 4s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 17/20\n",
            "820/820 [==============================] - 6s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "820/820 [==============================] - 5s 7ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "820/820 [==============================] - 6s 8ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "820/820 [==============================] - 4s 5ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting the GAN together"
      ],
      "metadata": {
        "id": "-h1o0Ycc02vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gan(generator, discriminator):\n",
        "  model = Sequential()\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
        "  return model"
      ],
      "metadata": {
        "id": "QSBHHSoV06Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator()\n",
        "discriminator = make_discriminator()\n",
        "gan = make_gan(generator, discriminator)"
      ],
      "metadata": {
        "id": "UNA2v9Mg1_Hw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}