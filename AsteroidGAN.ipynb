{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "HSWuwdFI0uqr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CmY_lJmazXLD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rand\n",
        "\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Activation, Conv1D, Dropout\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data pre-processing\n",
        "\n",
        "Due to the large size of dataset, many columns that are not needed and some rows containing null values have already been dropped"
      ],
      "metadata": {
        "id": "31YHj_tf0rBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"dataset.csv\")\n",
        "\n",
        "data = data.drop(['class', 'epoch_cal'], axis=1)\n",
        "data = data.dropna()\n",
        "data['neo'] = data['neo'].replace(('Y','N'), (1,0))\n",
        "data['pha'] = data['pha'].replace(('Y','N'), (1,0))\n",
        "\n",
        "#num_classes = data['class'].nunique()\n",
        "#classes = to_categorical(data['class'], num_classes)\n",
        "\n",
        "data = data.astype('float32')\n",
        "data /= 255\n",
        "\n",
        "print(data.shape)\n",
        "\n",
        "print(data.info)"
      ],
      "metadata": {
        "id": "NaRAER8R0w2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "709c4146-61c8-483f-9125-9c9177ed5b79"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(131124, 16)\n",
            "<bound method DataFrame.info of         neo  pha         H  diameter    albedo         e         a         q  \\\n",
            "0       0.0  0.0  0.013333  3.683922  0.000353  0.000298  0.010859  0.010034   \n",
            "1       0.0  0.0  0.016471  2.137255  0.000396  0.000902  0.010878  0.008376   \n",
            "2       0.0  0.0  0.020902  0.967043  0.000839  0.001008  0.010464  0.007775   \n",
            "3       0.0  0.0  0.011765  2.060392  0.001658  0.000348  0.009260  0.008439   \n",
            "4       0.0  0.0  0.027059  0.418427  0.001075  0.000749  0.010094  0.008167   \n",
            "...     ...  ...       ...       ...       ...       ...       ...       ...   \n",
            "573686  0.0  0.0  0.061176  0.017612  0.000094  0.000717  0.011518  0.009411   \n",
            "573687  0.0  0.0  0.060000  0.018020  0.000361  0.000289  0.012456  0.011538   \n",
            "573688  0.0  0.0  0.060392  0.016024  0.000455  0.000326  0.012663  0.011610   \n",
            "573689  0.0  0.0  0.064314  0.012820  0.000408  0.000827  0.012432  0.009811   \n",
            "573690  0.0  0.0  0.065882  0.008361  0.000243  0.000899  0.011454  0.008827   \n",
            "\n",
            "               i        om         w        ad         n        tp_cal  \\\n",
            "0       0.041545  0.314924  0.288618  0.011685  0.000839  79138.937500   \n",
            "1       0.136600  0.678528  1.216480  0.013379  0.000837  79140.085938   \n",
            "2       0.050945  0.666084  0.972809  0.013152  0.000887  79141.664062   \n",
            "3       0.028007  0.407101  0.591092  0.010082  0.001065  79139.257812   \n",
            "4       0.021049  0.555180  1.406464  0.012021  0.000936  79216.929688   \n",
            "...          ...       ...       ...       ...       ...           ...   \n",
            "573686  0.109669  0.674466  0.631966  0.013625  0.000768  79137.734375   \n",
            "573687  0.090955  0.168830  0.812197  0.013374  0.000683  79297.695312   \n",
            "573688  0.064425  0.247204  0.285630  0.013716  0.000666  79256.156250   \n",
            "573689  0.052861  0.314412  0.295110  0.015053  0.000685  79256.945312   \n",
            "573690  0.063265  0.714666  1.132945  0.014080  0.000774  79217.289062   \n",
            "\n",
            "             per      moid  \n",
            "0       6.600572  0.006254  \n",
            "1       6.617298  0.004840  \n",
            "2       6.243191  0.004056  \n",
            "3       5.197775  0.004469  \n",
            "4       5.915351  0.004297  \n",
            "...          ...       ...  \n",
            "573686  7.209965  0.005500  \n",
            "573687  8.108079  0.007620  \n",
            "573688  8.311297  0.007947  \n",
            "573689  8.085117  0.006092  \n",
            "573690  7.149632  0.005193  \n",
            "\n",
            "[131124 rows x 16 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training parameters\n"
      ],
      "metadata": {
        "id": "Im19QWilH2OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SHAPE = (16,1)\n",
        "OPTIMIZER = 'adam'\n",
        "LOSS = 'binary_crossentropy'\n",
        "METRICS = 'accuracy'\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "N_EPOCHS = 20\n",
        "VERBOSE = 1\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "metadata": {
        "id": "nfLDJUYCNVvI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator model\n",
        "18 outputs as there are 18 columns in the dataset"
      ],
      "metadata": {
        "id": "4yFogldYp9gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator(latent_dim):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(40, activation='relu', input_dim=latent_dim))\n",
        "  model.add(Dense(18, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "594ZrKNrNOVt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator model\n",
        "This is a binary CNN classifier, to determine whether the input is 'real' or not"
      ],
      "metadata": {
        "id": "nfK5IwLMrRPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator():\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv1D(64, kernel_size=5, padding='same', activation='relu', input_shape=INPUT_SHAPE))\n",
        "\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "yHa3MhT8rQvA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = make_discriminator()\n",
        "\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slp2HkUaOfmN",
        "outputId": "dee51ef6-2ed8-4313-cd6c-eebc53b772ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 16, 64)            384       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16, 1)             65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 449\n",
            "Trainable params: 449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_real = data\n",
        "y_real = np.ones(131124)"
      ],
      "metadata": {
        "id": "15lI4UpzrFUc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate random, fake samples for training discriminator"
      ],
      "metadata": {
        "id": "OZbVrKZ7e_fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fakes():\n",
        "  neo = rand.choice([1,0])\n",
        "  pha = rand.choice([1,0])\n",
        "  diameter = rand.uniform(0,100000)\n",
        "  albedo = rand.uniform(0,1)\n",
        "  e = rand.uniform(0, 0.002)\n",
        "  a = rand.uniform(0,1)\n",
        "  q = rand.uniform(0,1)\n",
        "  i = rand.uniform(0,1)\n",
        "  om = rand.uniform(0,1)\n",
        "  w = rand.uniform(0,2)\n",
        "  ad = rand.uniform(0,0.05)\n",
        "  n = rand.uniform(0,0.01)\n",
        "  tp_cal = rand.uniform(0,100000)\n",
        "  per = rand.uniform(0,10)\n",
        "  moid = rand.uniform(0,0.1)\n",
        "\n",
        "  X = np.array([neo, pha, diameter, albedo, e, a, q, i, om, w, ad, n, tp_cal, per, moid])\n",
        "  \n",
        "  return X"
      ],
      "metadata": {
        "id": "ZFO_oZ2de-yW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_fake = pd.DataFrame()\n",
        "for i in range(131124):\n",
        "  # fix below line - needs to add as a new row, not each value from the function as its own separate row\n",
        "  x_fake = pd.concat([x_fake, pd.Series(generate_fakes())])\n",
        "y_fake = np.zeros(131124)\n",
        "\n",
        "x = pd.concat([x_real, x_fake])\n",
        "y = pd.DataFrame(pd.Series(y_real))\n",
        "y = pd.concat([y, pd.Series(y_fake)])\n",
        "history = model.fit(x, y, epochs=N_EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "AkmvH66zSjMt",
        "outputId": "33de9762-242d-4e6c-fd44-4a854dda500d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e970c9b2cd28>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVALIDATION_SPLIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1850\u001b[0m             )\n\u001b[1;32m   1851\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1678387\n  y sizes: 262248\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting the GAN together"
      ],
      "metadata": {
        "id": "-h1o0Ycc02vZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_gan(generator, discriminator):\n",
        "  model = Sequential()\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "  model.compile(loss='binary_crossentropy', optimizer=OPTIMIZER)\n",
        "  return model"
      ],
      "metadata": {
        "id": "QSBHHSoV06Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator()\n",
        "discriminator = make_discriminator()\n",
        "gan = make_gan(generator, discriminator)"
      ],
      "metadata": {
        "id": "UNA2v9Mg1_Hw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}